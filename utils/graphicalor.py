import os
import torch
import torch.nn as nn
import transformers
from typing import Optional, Union, Callable, Tuple
try:
    from sentence_transformers import SentenceTransformer
    SENTENCE_TRANSFORMER_AVAILABLE = True
except ImportError:
    SENTENCE_TRANSFORMER_AVAILABLE = False
    print("Warning: sentence_transformers package not available. Some embedding features will be disabled.")
try:
    import torch_geometric as tg
    from torch_geometric.data import Data
    TORCH_GEOMETRIC_AVAILABLE = True
except ImportError:
    TORCH_GEOMETRIC_AVAILABLE = False
    print("Warning: torch_geometric package not available. Graph creation features will be disabled.")
    # 创建虚拟Data类
    class Data:
        def __init__(self, **kwargs):
            for key, value in kwargs.items():
                setattr(self, key, value)
try:
    from torch.fx import symbolic_trace
    TORCH_FX_AVAILABLE = True
except ImportError:
    TORCH_FX_AVAILABLE = False
    print("Warning: torch.fx not available. Graph visualization features will be limited.")

from utils.io import LogRedirectMixin, log
from utils.inspector import ModelInspector





class Graphicalor(LogRedirectMixin):
    def __init__(self, network:nn.Module, embedding_model:nn.Module = None, log_path:Optional[str] = None) -> None:
        super().__init__(log_path)
        self.network = network
        self.embedding_model = None
        
        if embedding_model:
            self.embedding_model = embedding_model
        elif SENTENCE_TRANSFORMER_AVAILABLE:
            log("No embedding model provided. Using default model: all-MiniLM-L6-v2.", level="WARNING")
            self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
        else:
            log("SentenceTransformer not available. Using dummy embedding.", level="WARNING")
            
        self.graph_A = None
        self.graph_B = []
        
        # 处理不存在torch.fx的情况
        if TORCH_FX_AVAILABLE:
            try:
                self.traced = symbolic_trace(self.network)
                self.extract_nodes_and_edges()
            except Exception as e:
                log(f"无法使用torch.fx进行符号跟踪: {e}", level="ERROR")
                self.traced = None
                self.nodes = {}
                self.edges = []
                self.graphb_nodes = []
        else:
            log("torch.fx不可用，无法进行符号跟踪", level="WARNING")
            self.traced = None
            self.nodes = {}
            self.edges = []
            self.graphb_nodes = []
    
    def _get_layer(self, name:str, verbose:bool=True) -> nn.Module:
        """Get a specific layer of model.

        Args:
            name (str): Layer name, split by dot(.), especially, can be 'all'.

        Returns:
            nn.Module: Target layer
        """
        if name == 'all':
            return self.network
        else:
            name_list = name.split('.')
            tar = self.network
            for i in name_list:
                try:
                    i = eval(i)
                except:
                    pass
                if isinstance(i, str):
                    tar = getattr(tar, i)
                elif isinstance(i, int):
                    try:
                        tar = tar[i]
                    except:
                        pass
            return tar

    def embedding_node(self, node:dict, verbose:bool=True):
        """
        Generate an embedding for a node based on its description.

        This function generates a natural language description for a given node 
        in a neural network based on its type and index in the network. The 
        description is then encoded into a vector representation using a pre-trained 
        embedding model.

        Args:
            node (dict): A dictionary containing node information. Expected keys:
                - 'name' (str): The name of the node.
                - 'type' (str): The type of the node (e.g., 'conv', 'relu').
                - 'index' (int): The index of the node in the network.
            verbose (bool, optional): Whether to log the description of the node. Defaults to True.

        Returns:
            np.ndarray: A vector representation of the node's description, generated by the embedding model.

        Notes:
            - The function uses a mapping dictionary to associate node types with 
            natural language descriptions.
            - If the node type is not in the mapping, it defaults to "Other operator."
            - The node index is converted to a human-readable ordinal string (e.g., 'first', 'second').
        
        Example:
            node = {'name': 'conv1', 'type': 'conv', 'index': 1}
            embedding = self.embedding_node(node)
        """

        mapping = {
            'conv': 'Convolution module',
            'linear': 'Linear module',
            'fc': 'Linear module',
            'batch_norm': 'BatchNorm activation',
            'relu': 'ReLU activation',
            'max_pool': 'MaxPool function',
            'avg_pool': 'AvgPool function',
            'dropout': 'Dropout function',
            'softmax': 'Softmax function',
            'sigmoid': 'Sigmoid function',
            'tanh': 'Tanh function',
            'other': 'Other operator',
        }
        
        def get_key(name):
            for k in mapping.keys():
                if k in str(name).lower():
                    return k
            else:
                return 'other'
        def get_index_string(ind):
            if ind == 0:
                return 'input'
            if ind == 1:
                return 'first'
            if ind == 2:
                return 'second'
            if ind == 3:
                return 'third'
            if ind > 3:
                return f"{ind}th"
            
        description = f"The {get_index_string(node['index'])} layer, which is a {mapping[get_key(node['type'])]}."
        if node['parameters'].sum() == 0:
            param_des = "No parameters."
        else:
            param_des = f"Its parameters mean is {node['parameters'][0].item()}, standard deviation is {node['parameters'][1].item()}, minimum is {node['parameters'][2].item()}, maximum is {node['parameters'][3].item()}, total number of parameters is {node['parameters'][4].item()}."
        description += param_des
        log(f"Name: {node['name']} Description: {description}.")
        
        # 根据是否有可用的嵌入模型，生成嵌入
        if self.embedding_model is not None and SENTENCE_TRANSFORMER_AVAILABLE:
            embedding = torch.tensor(self.embedding_model.encode(description))
        else:
            # 如果没有嵌入模型，使用简单的统计特征作为嵌入
            # 创建一个简单的嵌入表示（使用节点索引和参数统计作为特征）
            embedding = torch.zeros(10, dtype=torch.float32)  # 创建一个10维向量
            embedding[0] = node['index']  # 使用节点索引
            embedding[1:6] = node['parameters']  # 使用参数统计
            # 使用节点类型作为额外特征
            type_code = hash(str(node['type'])) % 1000 / 1000.0  # 将类型哈希转换为0-1之间的值
            embedding[6] = type_code
            log(f"使用统计特征作为嵌入，因为SentenceTransformer不可用", level="INFO")
            
        return embedding
    
    def extract_parameter_features(self, parameters:dict):
        """
        Extract fixed-length features from node parameters as a torch.Tensor.

        Args:
            parameters (dict): A dictionary of parameter tensors, typically from `module.named_parameters()`.

        Returns:
            torch.Tensor: A fixed-length tensor representing the parameters, 
                        or a zero tensor if no parameters are present.
        """
        if parameters is None:
            # No parameters: return zero vector
            return torch.zeros(5, dtype=torch.float32)
        all_params = parameters.flatten()
        mean = torch.mean(all_params)
        std = torch.std(all_params)
        min_val = torch.min(all_params)
        max_val = torch.max(all_params)
        param_count = torch.tensor(all_params.numel(), dtype=torch.float32)

        # Feature vector: [mean, std, min, max, param_count]
        return torch.tensor([mean, std, min_val, max_val, param_count], dtype=torch.float32)
    
    def extract_nodes_and_edges(self, verbose:bool=True):
        """Generate nodes and edges from the network.

        Args:
            verbose (bool, optional): If log verbose information. Defaults to True.

        Returns:
            nodes and edges.
        """
        nodes = {}
        graphb_nodes = []
        edges = []
        if verbose:
            log(f"Parsing network...")
        for ind, node in enumerate(self.traced.graph.nodes):
            setattr(node, 'name', node.name.replace('_', '.'))
            if node.op == "call_module":
                if 'fc' in node.name:
                    pass
                tar_node = self._get_layer(node.name)
                setattr(tar_node, 'name', node.name)
                setattr(tar_node, 'index', ind)
                try:
                    raw_params = torch.cat([param.view(-1) for param in tar_node.parameters()], dim=0)
                except:
                    raw_params = None
                paras = self.extract_parameter_features(raw_params)
                graphb_nodes.append(tar_node)
            else:
                paras = torch.zeros(5, dtype=torch.float32)
            nodes[node.name] = {
                "name": node.name,
                "index": ind,
                "type": node.target,
                "parameters": paras,
            }
            for arg in node.args:
                if isinstance(arg, torch.fx.Node):
                    edges.append((arg.name, node.name))
        for _, node in nodes.items():
            node['embedding'] = self.embedding_node(node, verbose=verbose)
        self.nodes = nodes
        self.edges = edges
        self.graphb_nodes = graphb_nodes
        return nodes, edges, graphb_nodes
    
    def grapha(self, verbose:bool = True):
        """
        Convert nodes and edges into a format suitable for graph neural networks.

        Args:
            nodes (dict): A dictionary of node information with embeddings.
            edges (list): A list of edges as tuples (source, target).
            verbose (bool, optional): If log verbose information. Defaults to True.

        Returns:
            dict: A dictionary containing node features and edge indices for GNN input.
        """
        if verbose:
            log("Converting nodes and edges to GNN input format...")
        nodes = self.nodes
        edges = self.edges
        # Extract node embeddings
        node_features = [nodes[node]['embedding'] for node in nodes]
        
        # Convert edge list to edge index tensor
        edge_index = torch.tensor(
            [[nodes[src]['index'], nodes[tgt]['index']] for src, tgt in edges],
            dtype=torch.long
        ).t()

        if verbose:
            log(f"Node features shape: {torch.stack(node_features).shape}")
            log(f"Edge index shape: {edge_index.shape}")

        nodes_and_edges = {
            "node_features": torch.stack(node_features),
            "edge_index": edge_index
        }
        self.graph_A = Data(
            x = nodes_and_edges['node_features'],
            edge_index = nodes_and_edges['edge_index']
        )
        return self.graph_A
    
    def mlp_to_graph(self, layer:torch.nn.Module) -> Data:
        
        """
        Convert an MLP layer into a graph representation.

        Args:
            layer(torch.nn.Module.Linear): Layer.

        Returns:
            Data: A torch_geometric.data.Data object representing the MLP layer.
        """
        assert isinstance(layer, nn.Linear), "Make sure layer is nn.Linear."
        weight = layer.weight
        bias = layer.bias
        in_features = weight.size(1)
        out_features = weight.size(0)

        # Nodes: input and output neurons
        num_nodes = in_features + out_features
        x = torch.zeros(num_nodes, 1)  # Nodes have no meaningful features, use zeros

        # Edges: connections between input and output neurons
        edge_index = []
        edge_attr = []

        for out_idx in range(out_features):
            for in_idx in range(in_features):
                edge_index.append([in_idx, in_features + out_idx])  # Edge: input -> output
                edge_attr.append([weight[out_idx, in_idx].item(), bias[out_idx].item()])  # Weight + bias

        edge_index = torch.tensor(edge_index, dtype=torch.long).t()  # Shape [2, num_edges]
        edge_attr = torch.tensor(edge_attr, dtype=torch.float32)  # Shape [num_edges, 2]

        # Create the Data object
        data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)
        try:
            data.layer_name = layer.name
            data.layer_index = layer.index
        except:
            pass
        return data

    def conv_to_graph(self, layer:torch.nn.Module) -> Data:
        """
        Convert an nn.Conv2d layer into a graph representation.

        Args:
            layer (torch.nn.Conv2d): A PyTorch Conv2d layer instance.

        Returns:
            Data: A torch_geometric.data.Data object representing the convolutional layer.
        """
        assert isinstance(layer, nn.Linear), "Make sure layer is nn.Linear."
        # Extract weights and biases
        weight = layer.weight  # Shape: [out_channels, in_channels, kh, kw]
        bias = layer.bias      # Shape: [out_channels] or None

        out_channels, in_channels, kh, kw = weight.shape

        # Nodes: input and output channels
        node_features = []

        # Calculate node features as statistics for each input channel
        for c in range(in_channels):
            channel_weights = weight[:, c, :, :].flatten()  # All weights related to this input channel
            node_features.append([channel_weights.mean().item(), channel_weights.std().item()])  # Mean and std

        node_features = torch.tensor(node_features, dtype=torch.float32)  # Shape [in_channels, 2]

        # Edges: connections between input and output channels
        edge_index = []
        edge_attr = []

        for out_channel in range(out_channels):
            for in_channel in range(in_channels):
                kernel_weights = weight[out_channel, in_channel].flatten()  # Kernel weights for this connection
                edge_index.append([in_channel, in_channels + out_channel])  # Edge: input -> output
                edge_attr.append(kernel_weights.tolist())  # Flattened kernel weights

        edge_index = torch.tensor(edge_index, dtype=torch.long).t()  # Shape [2, num_edges]
        edge_attr = torch.tensor(edge_attr, dtype=torch.float32)  # Shape [num_edges, kh * kw]

        # Add bias as an additional edge attribute (optional)
        if bias is not None:
            for out_channel in range(out_channels):
                edge_attr[out_channel::out_channels, :] += bias[out_channel].item()

        # Create the Data object
        data = Data(x=node_features, edge_index=edge_index, edge_attr=edge_attr)
        data.layer_name = layer.name  # Store the layer name for debugging
        data.layer_index = layer.index
        return data

    def graphb(self, verbose:bool=True):
        """
        Generate neuron-level graphs (graph_B) for all Conv2d and Linear layers in the model.

        This method iterates over `self.graphb_nodes`, which contains the nodes of the computational graph.
        For each node, if it is an instance of `nn.Conv2d` or `nn.Linear`, it generates a subgraph
        representing the layer at the neuron level and appends it to `self.graph_B`.

        Args:
            verbose (bool, optional): If True, logs additional information during processing. Defaults to True.

        Returns:
            list: A list of subgraphs, where each subgraph represents a Conv2d or Linear layer at the neuron level.
        """
        for node in self.graphb_nodes:
            if isinstance(node, nn.Conv2d):
                self.graph_B.append(self.conv_to_graph(node))
            elif isinstance(node, nn.Linear):
                self.graph_B.append(self.mlp_to_graph(node))
        return self.graph_B


class TransformerGraphicalor(ModelInspector):
    def __init__(self, model:nn.Module, tokenizer:Optional[Callable]=None, embedding_model:nn.Module=None, log_path:Optional[str]=None) -> None:
        """
        初始化TransformerGraphicalor，用于将Transformer模型表示为层次化图结构。
        
        Args:
            model (nn.Module): Transformer模型
            tokenizer (Optional[Callable], optional): 分词器. Defaults to None.
            embedding_model (nn.Module, optional): 用于生成节点语义表示的模型. Defaults to None.
            log_path (Optional[str], optional): 日志路径. Defaults to None.
        """
        super().__init__(model, tokenizer, log_path)
        # 确保模型是LM类型
        assert hasattr(model, '_model_type') and model._model_type == 'LM', "模型必须是Transformer语言模型类型"
        
        # 初始化图表示
        self.layer_wise_graph = None  # 层级图
        self.head_wise_graphs = []   # 每层的注意力头级图
        
        # 存储层和注意力头信息
        self.layers = []  # 模型的层列表
        self.layer_paths = []  # 层路径列表
        self.attention_heads = {}  # 每层的注意力头字典 {layer_idx: [heads]}
        
        # 设置默认相似度度量方法
        self.similarity_metric = "cosine"  # 可选: "cosine", "euclidean", "dot_product"
        
        # 保存embedding模型
        self.embedding_model = embedding_model
        
        if log_path:
            self.log_path = log_path
        
    def _get_statistics(self, tensor:torch.Tensor) -> torch.Tensor:
        """计算张量的统计特征

        Args:
            tensor (torch.Tensor): 输入张量

        Returns:
            torch.Tensor: 包含统计特征的张量 [mean, std, min, max, norm]
        """
        if tensor is None or tensor.numel() == 0:
            return torch.zeros(5, dtype=torch.float32)
        
        tensor = tensor.detach().float()
        mean = torch.mean(tensor)
        std = torch.std(tensor)
        min_val = torch.min(tensor)
        max_val = torch.max(tensor)
        norm = torch.norm(tensor)
        skewness = torch.mean(((tensor - torch.mean(tensor)) / torch.std(tensor)) ** 3)
        kurtosis = torch.mean(((tensor - torch.mean(tensor)) / torch.std(tensor)) ** 4)
        sparsity = torch.sum(tensor == 0.0) / tensor.numel()
        
        # 确保返回一维张量
        stats = torch.tensor([mean, std, min_val, max_val, norm, skewness, kurtosis, sparsity], dtype=torch.float32)
        return stats
    
    def _compute_similarity(self, tensor1:torch.Tensor, tensor2:torch.Tensor, method:str="cosine") -> float:
        """Compute the similarity between two tensors.

        Args:
            tensor1 (torch.Tensor): The first tensor.
            tensor2 (torch.Tensor): The second tensor.
            method (str, optional): The similarity metric. Defaults to "cosine".

        Returns:
            float: The similarity score.
        """
        # Ensure the input tensors are at least one-dimensional
        if tensor1.dim() == 0:
            tensor1 = tensor1.unsqueeze(0)
        if tensor2.dim() == 0:
            tensor2 = tensor2.unsqueeze(0)
            
        if tensor1.shape != tensor2.shape:
            # If the shapes are different, flatten them and then compute the similarity
            tensor1 = tensor1.flatten()
            tensor2 = tensor2.flatten()
            
        if method == "cosine":
            return torch.nn.functional.cosine_similarity(tensor1.flatten(), tensor2.flatten(), dim=0).item()
        elif method == "euclidean":
            return -torch.norm(tensor1 - tensor2).item()  # The negative Euclidean distance, the larger the value, the more similar
        elif method == "dot_product":
            return torch.dot(tensor1.flatten(), tensor2.flatten()).item()
        else:
            raise ValueError(f"Unsupported similarity metric: {method}")
    
    def _extract_layers(self, verbose:bool=True) -> list:
        """Extract the Transformer layers in the model, using the ModelInspector.get_layer method

        Args:
            verbose (bool, optional): Whether to print detailed information. Defaults to True.

        Returns:
            list: The list of Transformer layers
        """
        # Find all possible layer paths based on different model architectures
        layer_paths = []
        
        # Try common Transformer layer paths
        possible_paths = [
            "layers",             # Directly access the layers
            "model.layers",       # Access the layers through the model property
            "encoder.layers",     # Access the layers through the encoder
            "transformer.layers"  # Access the layers through the transformer
        ]
        
        for path in possible_paths:
            try:
                # Try to get the layers under the specified path
                layers_module = self.get_layer(path, verbose=False)
                
                # Check if it is a module containing multiple layers (ModuleList or similar structure)
                if hasattr(layers_module, "__len__"):
                    # Find the effective layer path
                    if verbose:
                        log(f"在路径 '{path}' 中找到层集合，包含 {len(layers_module)} 个层")
                    
                    # Build the path for each layer and add it to the list
                    for i in range(len(layers_module)):
                        layer_path = f"{path}.{i}"
                        layer_paths.append(layer_path)
                    
                    # After finding the effective path, break the loop
                    break
            except:
                # If the path does not exist, continue to try the next possible path
                continue
        
        # If the layers are not found through the predefined paths, try to search directly for modules containing attention mechanisms
        if not layer_paths:
            if verbose:
                log("没有在预定义路径中找到层，尝试直接搜索含有注意力机制的模块", level="WARNING")
            
            # Store the found layer paths
            found_paths = []
            
            # Search for modules containing attention mechanisms in all modules of the model
            def search_attention_layers(module, path=""):
                # Check if the current module contains self-attention and feed-forward networks (typical Transformer layer characteristics)
                has_attn = any("attn" in name.lower() for name, _ in module.named_children())
                has_ff = any(name in ["mlp", "feed_forward"] for name, _ in module.named_children())
                
                if has_attn and has_ff:
                    found_paths.append(path)
                
                # Recursively search for submodules
                for name, child in module.named_children():
                    child_path = f"{path}.{name}" if path else name
                    search_attention_layers(child, child_path)
            
            # Start searching from the root module
            search_attention_layers(self.model)
            
            # Use the found layer paths
            layer_paths = found_paths
            
            if verbose and layer_paths:
                log(f"Through searching, found {len(layer_paths)} possible Transformer layers")
        
        # Store the layer paths and layer instances
        self.layer_paths = layer_paths
        self.layers = [self.get_layer(path, verbose=False) for path in layer_paths]
        
        if verbose:
            log(f"Finally extracted {len(self.layers)} Transformer layers")
        
        return self.layers
    
    def _extract_attention_heads(self, layer_path:str, layer_idx:int, verbose:bool=True) -> list:
        """Extract the attention heads from the Transformer layer, using the ModelInspector method

        Args:
            layer_path (str): The path of the layer
            layer_idx (int): The index of the layer
            verbose (bool, optional): Whether to print detailed information. Defaults to True.

        Returns:
            list: The list of attention heads or parameters
        """
        # For different model architectures, the way to extract attention heads may be different
        try:
            # Get the layer instance
            layer = self.get_layer(layer_path, verbose=False)
            
            # Get the self-attention module
            attn_path = None
            if hasattr(layer, 'self_attn'):
                attn_path = f"{layer_path}.self_attn"
            else:
                # Try to find the attention module
                for name, _ in layer.named_children():
                    if 'attn' in name.lower() or 'attention' in name.lower():
                        attn_path = f"{layer_path}.{name}"
                        break
            
            if attn_path is None:
                if verbose:
                    log(f"No attention module found in layer {layer_path}", level="WARNING")
                return []
            
            # Get the attention module instance
            attn = self.get_layer(attn_path, verbose=False)
            
            # Get the number of attention heads
            if isinstance(attn, transformers.models.llama.modeling_llama.LlamaAttention) or isinstance(attn, transformers.models.qwen2.modeling_qwen2.Qwen2Attention):
                config = getattr(attn, 'config')
                n_heads = config.num_attention_heads
            else:
                n_heads = getattr(attn, 'num_heads', None)  # TODO: Need make it more general
            if n_heads is None:
                n_heads = getattr(attn, 'n_head', None)
            if n_heads is None:
                n_heads = getattr(attn, 'num_attention_heads', None)
            
            if n_heads is None:
                if verbose:
                    log(f"Cannot determine the number of attention heads in layer {layer_path}", level="WARNING")
                return []
                
            if verbose:
                log(f"Layer {layer_path} contains {n_heads} attention heads")
            
            # Extract attention parameters
            heads_data = []
            
            # Use the ModelInspector's get_para method to get parameters
            all_params = {}
            for name, param in attn.named_parameters():
                if any(x in name for x in ['q_proj', 'k_proj', 'v_proj', 'query', 'key', 'value']):
                    all_params[name] = param
            
            # Try to assign parameters to each head
            for name, param in all_params.items():
                if 'weight' in name and param.dim() == 2:
                    try:
                        hidden_size = param.shape[0]
                        head_dim = hidden_size // n_heads
                        # Reshape the parameters to assign to each head
                        param_reshaped = param.view(n_heads, head_dim, -1)
                        
                        for head_idx in range(n_heads):
                            if len(heads_data) <= head_idx:
                                heads_data.append({})
                            heads_data[head_idx][name] = param_reshaped[head_idx]
                    except Exception as e:
                        if verbose:
                            log(f"Cannot assign parameter {name} to attention head: {e}", level="WARNING")
                            
            # If the model is trained, try to get gradients
            if self.status == 'trained':
                # Use the ModelInspector's get_grad method to get gradients
                all_grads = self.get_grad(attn_path, type_='dict', verbose=False)
                
                for name, grad in all_grads.items():
                    if any(x in name for x in ['q_proj', 'k_proj', 'v_proj', 'query', 'key', 'value']):
                        if 'weight' in name and grad.dim() == 2:
                            try:
                                hidden_size = grad.shape[0]
                                head_dim = hidden_size // n_heads
                                # Reshape the gradients to assign to each head
                                grad_reshaped = grad.view(n_heads, head_dim, -1)
                                
                                for head_idx in range(n_heads):
                                    if len(heads_data) <= head_idx:
                                        heads_data.append({})
                                    heads_data[head_idx][f"{name}_grad"] = grad_reshaped[head_idx]
                            except Exception as e:
                                if verbose:
                                    log(f"Cannot assign gradient {name} to attention head: {e}", level="WARNING")
            
            # If no head information is successfully extracted
            if not heads_data:
                if verbose:
                    log(f"Cannot extract the attention head details from layer {layer_path}, returning the full module", level="WARNING")
                
                # Create a simple head representation, each head using the same parameters
                param_list = self.get_para(attn_path, type_='list', verbose=False)
                grad_dict = self.get_grad(attn_path, type_='dict', verbose=False) if self.status == 'trained' else {}
                
                # Create a simple head representation, each head using the same parameters
                for head_idx in range(n_heads):
                    head_data = {}
                    for i, param in enumerate(param_list):
                        head_data[f"param_{i}"] = param
                    
                    for name, grad in grad_dict.items():
                        head_data[f"{name}_grad"] = grad
                    
                    heads_data.append(head_data)
                
                if verbose:
                    log(f"Created {n_heads} generic attention head representations for layer {layer_path}")
            
            # Store the extracted head information
            self.attention_heads[layer_idx] = heads_data
            return heads_data
            
        except Exception as e:
            log(f"Error extracting attention heads from layer {layer_path}: {e}", level="ERROR")
            return []
    
    def _get_embedding(self, tensor_dict) -> torch.Tensor:
        """Use the embedding model to get the low-dimensional representation of the input tensor
        
        Args:
            tensor_dict (dict): The dictionary containing the tensors

        Returns:
            torch.Tensor: The embedding vector, return an empty tensor if no embedding model
        """
        if self.embedding_model is None:
            return torch.tensor([])
        
        # Flatten the parameters and combine them as input
        flattened_tensors = []
        for name, tensor in tensor_dict.items():
            if isinstance(tensor, torch.Tensor) and 'grad' not in name:
                flattened_tensors.append(tensor.flatten())
        
        if not flattened_tensors:
            return torch.tensor([])
        
        # Combine all tensors
        combined_tensor = torch.cat(flattened_tensors)
        
        # Get the low-dimensional representation through the embedding model
        with torch.no_grad():
            embedding = self.embedding_model(combined_tensor.unsqueeze(0))
            # Assuming the embedding model is a VAE, it may return multiple values, we only use the latent representation
            if isinstance(embedding, tuple):
                embedding = embedding[0]  # Take the latent representation of the VAE
            
            # Ensure the returned tensor is one-dimensional
            if embedding.dim() > 1:
                embedding = embedding.squeeze(0)
            
        return embedding
    
    def _get_layer_representation(self, layer_path:str, layer_idx:int) -> dict:
        """Get the representation of the layer, using the ModelInspector method

        Args:
            layer_path (str): The path of the layer
            layer_idx (int): The index of the layer

        Returns:
            dict: The representation of the layer
        """
        # Get the layer parameters
        params_list = self.get_para(layer_path, type_='list', verbose=False)
        
        # Calculate the parameter statistics
        params_stats = {}
        for i, param in enumerate(params_list):
            param_name = f"param_{i}"
            params_stats[param_name] = self._get_statistics(param)
        
        # Get the gradient statistics
        grads_stats = {}
        if self.status == 'trained':  # If the model has been calibrated
            grad_dict = self.get_grad(layer_path, type_='dict', verbose=False)
            for name, grad in grad_dict.items():
                grads_stats[name] = self._get_statistics(grad)
        
        # Summarize the statistics
        all_param_stats = torch.cat(list(params_stats.values())) if params_stats else torch.zeros(0)
        all_grad_stats = torch.cat(list(grads_stats.values())) if grads_stats else torch.zeros(0)
        
        # Get the embedding representation of the parameters
        param_dict = {f"param_{i}": param for i, param in enumerate(params_list)}
        embedding = self._get_embedding(param_dict)
        
        layer_info = {
            'index': layer_idx,
            'path': layer_path,
            'name': f"transformer_layer_{layer_idx}",
            'params_stats': params_stats,
            'grads_stats': grads_stats,
            'params_stats_combined': torch.mean(all_param_stats, dim=0) if len(all_param_stats) > 0 else torch.zeros(5),
            'grads_stats_combined': torch.mean(all_grad_stats, dim=0) if len(all_grad_stats) > 0 else torch.zeros(5),
            'embedding': embedding,
        }
        
        return layer_info
    
    def _get_head_representation(self, head_data:dict, head_idx:int, layer_idx:int) -> dict:
        """获取注意力头的表示

        Args:
            head_data (dict): 注意力头数据
            head_idx (int): 头索引
            layer_idx (int): 层索引

        Returns:
            dict: 注意力头的表示
        """
        # 分离参数和梯度
        params_stats = {}
        grads_stats = {}
        params_dict = {}  # 用于嵌入
        
        for name, tensor in head_data.items():
            if name.endswith('_grad'):
                # 这是梯度
                grads_stats[name] = self._get_statistics(tensor)
            else:
                # 这是参数
                params_stats[name] = self._get_statistics(tensor)
                params_dict[name] = tensor  # 存储原始参数用于嵌入
        
        # 汇总统计信息
        all_param_stats = torch.cat(list(params_stats.values())) if params_stats else torch.zeros(0)
        all_grad_stats = torch.cat(list(grads_stats.values())) if grads_stats else torch.zeros(0)
        
        # 获取参数的嵌入表示
        embedding = self._get_embedding(params_dict)
        
        head_info = {
            'layer_index': layer_idx,
            'head_index': head_idx,
            'name': f"layer_{layer_idx}_head_{head_idx}",
            'params_stats': params_stats,
            'grads_stats': grads_stats,
            'params_stats_combined': torch.mean(all_param_stats, dim=0) if len(all_param_stats) > 0 else torch.zeros(5),
            'grads_stats_combined': torch.mean(all_grad_stats, dim=0) if len(all_grad_stats) > 0 else torch.zeros(5),
            'embedding': embedding,
        }
        
        return head_info
    
    def build_layer_wise_graph(self, similarity_metric:str="cosine", verbose:bool=True) -> Data:
        """构建层级图

        Args:
            similarity_metric (str, optional): 相似度度量方法. Defaults to "cosine".
            verbose (bool, optional): 是否打印详细信息. Defaults to True.

        Returns:
            Data: 层级图
        """
        # 设置相似度度量方法
        self.similarity_metric = similarity_metric
        
        # 提取所有层
        layers = self._extract_layers(verbose=verbose)
        if not layers:
            log("未找到模型层，无法构建层级图", level="ERROR")
            return None
        
        # 为每层获取表示
        layer_nodes = []
        for idx, layer_path in enumerate(self.layer_paths):
            layer_info = self._get_layer_representation(layer_path, idx)
            layer_nodes.append(layer_info)
        
        # 构建节点特征
        node_features = []
        for node in layer_nodes:
            # 确保张量至少是一维的，然后再连接
            params_stats = node['params_stats_combined']
            grads_stats = node['grads_stats_combined']
            
            # 检查并确保张量是一维的
            if params_stats.dim() == 0:
                params_stats = params_stats.unsqueeze(0)
            if grads_stats.dim() == 0:
                grads_stats = grads_stats.unsqueeze(0)
                
            # 结合参数和梯度统计信息
            node_feat = torch.cat([params_stats, grads_stats])
            
            # 增加嵌入表示（如果可用）
            embedding = node.get('embedding', torch.tensor([]))
            if embedding.numel() > 0:  # 检查嵌入向量是否非空
                node_feat = torch.cat([node_feat, embedding])
            
            node_features.append(node_feat)
        
        node_features = torch.stack(node_features)  # (n_layers, 2)
        
        # 构建连接边
        # 对于Transformer模型，默认每层连接到下一层
        edge_index = []
        for i in range(len(layers) - 1):
            edge_index.append([i, i+1])  # 前向连接
        
        # 添加跳跃连接（如果需要）
        # 假设每层可能也有跳跃连接到之后的层
        for i in range(len(layers)):
            for j in range(i+2, len(layers)):
                # 计算层间相似度，如果相似度高于阈值，添加跳跃连接
                sim = self._compute_similarity(
                    node_features[i], 
                    node_features[j],
                    method=similarity_metric
                )
                threshold = 0.95 if similarity_metric == "cosine" else 0.5  # 根据度量方法调整阈值
                if sim > threshold:
                    edge_index.append([i, j])
                    if verbose:
                        log(f"添加跳跃连接: 层 {i} -> 层 {j}, 相似度: {sim:.4f}")
        
        # 创建图数据
        edge_index = torch.tensor(edge_index, dtype=torch.long).t()
        self.layer_wise_graph = Data(x=node_features, edge_index=edge_index)
        
        if verbose:
            log(f"构建层级图完成，节点数: {len(layer_nodes)}，边数: {edge_index.shape[1]}")
        
        return self.layer_wise_graph
    
    def build_head_wise_graphs(self, similarity_metric:str="cosine", similarity_threshold:float=0.0, verbose:bool=True) -> list:
        """为每层构建注意力头级图

        Args:
            similarity_metric (str, optional): 相似度度量方法. Defaults to "cosine".
            similarity_threshold (float, optional): 相似度阈值，只有当两个头的相似度超过此值时才创建边. Defaults to 0.0.
            verbose (bool, optional): 是否打印详细信息. Defaults to True.

        Returns:
            list: 每层的注意力头级图
        """
        # 设置相似度度量方法
        self.similarity_metric = similarity_metric
        
        # 确保已提取层
        if not hasattr(self, 'layer_paths') or not self.layer_paths:
            self._extract_layers(verbose=verbose)
        
        # 清空现有头级图
        self.head_wise_graphs = []
        
        # 为每层构建头级图
        for layer_idx, layer_path in enumerate(self.layer_paths):
            # 提取注意力头
            heads = self._extract_attention_heads(layer_path, layer_idx, verbose=verbose)
            if not heads:
                if verbose:
                    log(f"层 {layer_path} 没有提取到注意力头，跳过构建头级图", level="WARNING")
                continue
            
            # 为每个头获取表示
            head_nodes = []
            for head_idx, head_data in enumerate(heads):
                head_info = self._get_head_representation(head_data, head_idx, layer_idx)
                head_nodes.append(head_info)
            
            # 构建节点特征
            node_features = []
            for node in head_nodes:
                # 确保张量至少是一维的
                params_stats = node['params_stats_combined']
                grads_stats = node['grads_stats_combined']
                
                # 检查并确保张量是一维的
                if params_stats.dim() == 0:
                    params_stats = params_stats.unsqueeze(0)
                if grads_stats.dim() == 0:
                    grads_stats = grads_stats.unsqueeze(0)
                
                # 结合参数和梯度统计信息
                node_feat = torch.cat([params_stats, grads_stats])
                
                # 增加嵌入表示（如果可用）
                embedding = node.get('embedding', torch.tensor([]))
                if embedding.numel() > 0:  # 检查嵌入向量是否非空
                    node_feat = torch.cat([node_feat, embedding])
                
                node_features.append(node_feat)
            
            if not node_features:
                if verbose:
                    log(f"层 {layer_path} 没有有效的注意力头特征", level="WARNING")
                continue
                
            node_features = torch.stack(node_features)
            
            # 构建头间连接
            edge_index = []
            edge_attr = []
            
            # 计算所有头之间的相似度，只有高于阈值的才创建边
            edge_count = 0
            for i in range(len(head_nodes)):
                for j in range(i+1, len(head_nodes)):
                    sim = self._compute_similarity(
                        node_features[i], 
                        node_features[j],
                        method=similarity_metric
                    )
                    # 只有相似度高于阈值时才添加边
                    if sim > similarity_threshold:
                        # 添加双向边
                        edge_index.append([i, j])
                        edge_index.append([j, i])
                        edge_attr.append(sim)
                        edge_attr.append(sim)
                        edge_count += 2
            
            # 创建图数据
            if edge_index:
                edge_index = torch.tensor(edge_index, dtype=torch.long).t()
                edge_attr = torch.tensor(edge_attr, dtype=torch.float32).unsqueeze(1)
                head_graph = Data(
                    x=node_features, 
                    edge_index=edge_index,
                    edge_attr=edge_attr
                )
                head_graph.layer_index = layer_idx
                head_graph.layer_path = layer_path
                
                self.head_wise_graphs.append(head_graph)
                
                if verbose:
                    log(f"层 {layer_path} 的头级图构建完成，节点数: {len(head_nodes)}，边数: {edge_index.shape[1]}")
                    log(f"阈值: {similarity_threshold}，过滤前潜在边数: {len(head_nodes) * (len(head_nodes) - 1)}，实际创建边数: {edge_count}")
            else:
                if verbose:
                    log(f"层 {layer_path} 的头级图没有满足阈值 {similarity_threshold} 的边", level="WARNING")
        
        return self.head_wise_graphs
    
    def build_hierarchical_graph(self, similarity_metric:str="cosine", similarity_threshold:float=0.0, verbose:bool=True) -> tuple:
        """构建层次化图表示（包括层级图和所有头级图）

        Args:
            similarity_metric (str, optional): 相似度度量方法. Defaults to "cosine".
            similarity_threshold (float, optional): 注意力头相似度阈值，只有当两个头的相似度超过此值时才创建边. Defaults to 0.0.
            verbose (bool, optional): 是否打印详细信息. Defaults to True.

        Returns:
            tuple: (层级图, 头级图列表)
        """
        # 构建层级图
        layer_graph = self.build_layer_wise_graph(similarity_metric, verbose)
        
        # 构建所有层的头级图
        head_graphs = self.build_head_wise_graphs(similarity_metric, similarity_threshold, verbose)
        
        return layer_graph, head_graphs
    
    def visualize_layer_graph(self, save_path:str="layer_graph.png"):
        """可视化层级图

        Args:
            save_path (str, optional): 保存路径. Defaults to "layer_graph.png".
        """
        if self.layer_wise_graph is None:
            log("请先调用build_layer_wise_graph构建层级图", level="ERROR")
            return
        
        try:
            import networkx as nx
            import matplotlib.pyplot as plt
            
            # 转换为networkx图进行可视化
            G = tg.utils.convert.to_networkx(self.layer_wise_graph)
            
            plt.figure(figsize=(10, 8))
            pos = nx.spring_layout(G)
            nx.draw_networkx(G, pos, with_labels=True, node_color='lightblue', 
                             node_size=500, font_size=10, font_weight='bold')
            
            plt.title("Transformer Layer Graph")
            plt.axis('off')
            plt.savefig(os.path.join(self.log_path if hasattr(self, 'log_path') else '.', save_path))
            plt.close()
            
            log(f"层级图已保存至 {os.path.join(self.log_path if hasattr(self, 'log_path') else '.', save_path)}")
            
        except ImportError:
            log("请安装networkx和matplotlib以支持图可视化", level="ERROR")
    
    def visualize_head_graph(self, layer_idx:int=0, save_path:str=None):
        """可视化指定层的头级图

        Args:
            layer_idx (int, optional): 层索引. Defaults to 0.
            save_path (str, optional): 保存路径. Defaults to None.
        """
        if not self.head_wise_graphs:
            log("请先调用build_head_wise_graphs构建头级图", level="ERROR")
            return
        
        # 查找对应层的头级图
        target_graph = None
        for graph in self.head_wise_graphs:
            if graph.layer_index == layer_idx:
                target_graph = graph
                break
        
        if target_graph is None:
            log(f"未找到层 {layer_idx} 的头级图", level="ERROR")
            return
        
        try:
            import networkx as nx
            import matplotlib.pyplot as plt
            
            # 转换为networkx图进行可视化
            G = tg.utils.convert.to_networkx(target_graph)
            
            # 设置边权重（相似度）作为标签
            edge_labels = {}
            for i, (u, v) in enumerate(target_graph.edge_index.t().tolist()):
                edge_labels[(u, v)] = f"{target_graph.edge_attr[i].item():.2f}"
            
            plt.figure(figsize=(10, 8))
            pos = nx.circular_layout(G)
            nx.draw_networkx(G, pos, with_labels=True, node_color='lightgreen', 
                             node_size=500, font_size=10, font_weight='bold')
            nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=8)
            
            plt.title(f"Attention Heads Graph for Layer {layer_idx}")
            plt.axis('off')
            
            if save_path is None:
                save_path = f"head_graph_layer_{layer_idx}.png"
            
            plt.savefig(os.path.join(self.log_path if hasattr(self, 'log_path') else '.', save_path))
            plt.close()
            
            log(f"层 {layer_idx} 的头级图已保存至 {os.path.join(self.log_path if hasattr(self, 'log_path') else '.', save_path)}")
            
        except ImportError:
            log("请安装networkx和matplotlib以支持图可视化", level="ERROR")


if __name__ == '__main__':
    # from models import get_model
    # resnet50 = get_model('resnet50').cuda()
    # g = Graphicalor(resnet50)
    # graph = g.grapha()
    # print(graph)
    # 加载语言模型
    print("="*50)
    print("Loading Language Model")
    print("="*50)
    from models import get_model
    from config.config import CONF
    llama_model, tokenizer = get_model('Qwen2.5-3B', cache_dir=CONF.cache_dir, add_padding_token=True)

    # 创建TransformerGraphicalor实例
    transformer_graph = TransformerGraphicalor(llama_model, tokenizer)

    # 可选：校准模型以获取梯度
    from data import get_dataset
    dataset = get_dataset('imdb')
    transformer_graph.calibrate(dataset, task_type='sequence_classification')

    # 构建层次化图表示
    print("\n开始构建层次化图表示...")
    layer_graph, head_graphs = transformer_graph.build_hierarchical_graph(
        similarity_metric="cosine",
        similarity_threshold=0.9,  # 设置相似度阈值为0.9
        verbose=True
    )

    # 可视化
    transformer_graph.visualize_layer_graph("llama_layer_graph.png")
    for i in range(min(3, len(transformer_graph.layer_paths))):  # 可视化前三层
        transformer_graph.visualize_head_graph(layer_idx=i)